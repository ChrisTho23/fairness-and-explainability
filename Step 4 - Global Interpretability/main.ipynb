{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Global Interpretability\n",
    "\n",
    "In this first part, we aim to interpret our blackbox model globally :\n",
    " \n",
    "1. By implementing one surrogate method to interpret our black-box model\n",
    "2. By implementing two other post-hoc global methods (PDP, ALE, etc) to interpret your black-box model. \n",
    "3. We finally compare then compare the two sets of results, blackbox and whitebox model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from PyALE import ale\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "from pygam import LogisticGAM, s,  f\n",
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implementing a Surrogate model to interpret our blackbox model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Loading the Catboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('../Dataset/df_processed.csv')\n",
    "X = df.drop('Grade', axis=1).copy()\n",
    "y = df['Grade'].copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Load the model from the file\n",
    "model_filename = '../Models/blackbox_model.pkl'\n",
    "blackbox_model = joblib.load(model_filename)\n",
    "y_pred_blackbox = blackbox_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blackbox_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Implementing and Saving the surrogate model: Tuned decision tree\n",
    "\n",
    "We chose to implement a decision tree model with tuned parameters, for the following reasons: \n",
    "- it's very easy to interpret, and by plotting the decision logic in the form of a binary tree, we understand what the decision logic behind the model is.\n",
    "- the catboost model is also a tree-based model, so it's legitimate to choose a surrogate model based on decision tree logic.\n",
    "\n",
    "Furthermore, we note that our blackbox model has a feature depth of 7, so we arbitrarily choose to set the depth of our surrogate decision tree model also to 7 so that it best explains the performance of our catboost blackbox model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('../Dataset/df_processed.csv')\n",
    "X = df.drop('Grade', axis=1).copy()\n",
    "y = df['Grade'].copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Apply a numerical mapping for categorical columns\n",
    "categorical_mapping = {\n",
    "    'Gender': {'female': 0, 'male': 1},\n",
    "    'EthnicGroup': {'group A': 0, 'group B': 1, 'group C': 2, 'group D': 3, 'group E': 4},\n",
    "    'ParentEduc': {'some high school': 0, 'high school': 1, \"associate's degree\": 2, 'some college': 3, \"bachelor's degree\": 4, \"master's degree\": 5},\n",
    "    'LunchType': {'standard': 0, 'free/reduced': 1},\n",
    "    'ParentMaritalStatus': {'widowed': 0, 'divorced': 1, 'single': 2, 'married': 3},\n",
    "    'PracticeSport': {'sometimes': 0, 'regularly': 1, 'never': 2},\n",
    "    'WklyStudyHours': {'Less than 5 hours': 0, 'Between 5-10 hours': 1, 'More than 10 hours': 2}\n",
    "}\n",
    "\n",
    "columns_to_map = list(categorical_mapping.keys())\n",
    "X_train[columns_to_map] = X_train[columns_to_map].replace(categorical_mapping)\n",
    "X_test[columns_to_map] = X_test[columns_to_map].replace(categorical_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function to minimize\n",
    "def objective(params):\n",
    "    decision_tree = DecisionTreeClassifier(\n",
    "        random_state=42,\n",
    "        min_samples_leaf=params['min_samples_leaf'],\n",
    "        max_leaf_nodes=params['max_leaf_nodes'],\n",
    "        min_samples_split=params['min_samples_split'],\n",
    "        min_weight_fraction_leaf=params['min_weight_fraction_leaf'],\n",
    "    )\n",
    "    score = -cross_val_score(decision_tree, X_train, y_train, cv=5, scoring='accuracy').mean()\n",
    "    return score\n",
    "\n",
    "# Space for hyperparameters gridsearch\n",
    "space = {\n",
    "    'min_samples_leaf': hp.randint('min_samples_leaf', 1, 11),\n",
    "    'max_leaf_nodes': hp.randint('max_leaf_nodes', 2, 100),\n",
    "    'min_samples_split': hp.randint('min_samples_split', 2, 21),\n",
    "    'min_weight_fraction_leaf': hp.uniform('min_weight_fraction_leaf', 0.0, 0.5),\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective, \n",
    "            space=space, \n",
    "            algo=tpe.suggest, \n",
    "            max_evals=400, \n",
    "            trials=trials, \n",
    "            rstate=np.random.default_rng(seed=42))\n",
    "\n",
    "print(\"Best Hyperparameters:\", best)\n",
    "\n",
    "best_min_samples_leaf = int(best['min_samples_leaf'])\n",
    "best_max_leaf_nodes = int(best['max_leaf_nodes'])\n",
    "best_min_samples_split = int(best['min_samples_split'])\n",
    "best_min_weight_fraction_leaf = best['min_weight_fraction_leaf']\n",
    "\n",
    "# Fit the model and make predictions\n",
    "best_decision_tree = DecisionTreeClassifier(\n",
    "    random_state=42,\n",
    "    max_depth=7, # Set to 7 as our black-box catboost model\n",
    "    min_samples_leaf=best_min_samples_leaf,\n",
    "    max_leaf_nodes=best_max_leaf_nodes,\n",
    "    min_samples_split=best_min_samples_split,\n",
    "    min_weight_fraction_leaf=best_min_weight_fraction_leaf,\n",
    ")\n",
    "best_decision_tree.fit(X_train, y_train)\n",
    "y_test_pred = best_decision_tree.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy_surrogate = accuracy_score(y_test, y_test_pred)\n",
    "precision_surrogate = precision_score(y_test, y_test_pred)\n",
    "recall_surrogate = recall_score(y_test, y_test_pred)\n",
    "f1_surrogate = f1_score(y_test, y_test_pred)\n",
    "\n",
    "print('Accuracy test score : ', np.round(accuracy_surrogate, 4)*100, '%')\n",
    "print('Precision test score : ', np.round(precision_surrogate, 4)*100, '%')\n",
    "print('Recall test score : ', np.round(recall_surrogate, 4)*100, '%')\n",
    "print('F1 test score : ', np.round(f1_surrogate, 4)*100, '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the surrogate model \n",
    "model_filename = '../Models/surrogate_model.pkl'\n",
    "joblib.dump(best_decision_tree, model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Using Surrogate model to interpret Black-box model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from the best_decision_tree model\n",
    "feature_importances = best_decision_tree.feature_importances_\n",
    "importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importances for the top 10 features\n",
    "colors = cm.rainbow(np.linspace(0, 1, len(importance_df)))\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'], color=colors)\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Features Importance for\\n Surrogate model (Decision Tree)')\n",
    "plt.show()\n",
    "\n",
    "# Features Importance plot du blackbox model\n",
    "columns_names = X.columns.to_list()\n",
    "weights_blackbox = blackbox_model.feature_importances_\n",
    "indices = np.argsort(weights_blackbox)[::-1]\n",
    "sorted_weights = [weights_blackbox[i] for i in indices]\n",
    "sorted_columns = [columns_names[i] for i in indices]\n",
    "\n",
    "# Create the figure\n",
    "plt.figure(figsize=(6, 3.5))\n",
    "colors = plt.get_cmap(\"rainbow\")(np.linspace(0, 1, len(sorted_columns)))\n",
    "plt.barh(sorted_columns, sorted_weights, color=colors)\n",
    "plt.xticks(fontsize=7)  \n",
    "plt.xlabel('Weights')\n",
    "plt.title('Features Importances plot of the Blackbox model \\n')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree logic plot  \n",
    "plt.figure(figsize=(30, 15)) \n",
    "plot_tree(\n",
    "    best_decision_tree,\n",
    "    max_depth=4,\n",
    "    filled=True,\n",
    "    feature_names=X_train.columns.tolist(),\n",
    "    class_names=['0', '1'],\n",
    "    fontsize=10,  \n",
    "    impurity=False  \n",
    ")\n",
    "plt.title(\"Decision Tree Visualization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are fairly consistent: \n",
    "\n",
    "As we set the maxdepth of the surrogate model to 7, the decision tree evaluates the three variables School_Bus, isFirstChild and PracticeSport as insignificant (no associated coefficient). The blackbox model is fairly consistent on this point, rating School_Bus and isFirstChild as the two variables with the least impact on the result. However, it places the variable PracticeSport as the 5th variable with the highest weight, which can't quite be explained by our algorithm. When using logic, it's not easy to decide whether or not this variable is impactful, as we know that sport can both be very beneficial for achieving greater efficiency at work, but can also be considered time-consuming and therefore reduce the time allocated to revision. So the link isn't necessarily direct, and it's hard to interpret this contrary result between the black-box and surrogate model.\n",
    "\n",
    "Apart from that, it's good to see that the most impactful features of the decision tree model (surrogate model) are found in the most important features of the Catboost black-box model: **LunchType**, **ParentEduc** and **TestPrep** are among the most impactful feautres.\n",
    "\n",
    "It's pretty much in line with what we've been thinking: lunchType inevitably impacts the quality of students' work (those who eat at the canteen certainly save themselves commuting time and therefore have more time to work). Parents with a higher level of education certainly encourage their children to work harder. And finally, those with 1 in TestPrep are obviously more prepared, which may justify higher marks.\n",
    "\n",
    "However, the decision tree is rather limited because, as we said earlier, it is quite prone to overfitting and will not give the same results depending on the training dataset. It may therefore be interesting to interpret our black-box model using our white-box model defined in part 3, the tuned GAM model, which can also act as an overrogate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the file\n",
    "model_filename = '../Models/whitebox_model.pkl'\n",
    "whitebox_model = joblib.load(model_filename)\n",
    "whitebox_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitebox_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the GAM model judges 4 features as insignificant (i.e. with a p-value greater than 5%): \n",
    "- f(5) : ParentMaritalStatus\n",
    "- f(7) : IsFirstChild\n",
    "- s(8) : NrSiblings\n",
    "- f(9) : School_Bus\n",
    "\n",
    "These results fully confirm the results of the first surrogate model, the decision tree, which considers these three features to have the least impact on predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implementing two other post-hoc global methods (PDP, ALE) to interpret Black-box model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A. Partial Dependency Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot partial dependency plots\n",
    "titles = X_train.columns[0:11]\n",
    "plt.figure()\n",
    "\n",
    "fig, axs = plt.subplots(4, 3, figsize=(15, 12))  \n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    if i < 11:  \n",
    "        XX = whitebox_model.generate_X_grid(term=i)\n",
    "        ax.plot(XX[:, i], whitebox_model.partial_dependence(term=i, X=XX))\n",
    "        ax.plot(XX[:, i], whitebox_model.partial_dependence(term=i, X=XX, width=0.8)[1], c='r', ls='--')\n",
    "        if i == 0:\n",
    "            ax.set_ylim(-30, 30)\n",
    "        ax.set_title(titles[i])\n",
    "\n",
    "plt.suptitle('Partial Dependency plots for logistic GAM model 1', fontsize=20)\n",
    "plt.subplots_adjust(top=1) \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These partial dependency plots fully confirm the results of the first surrogate model, the decision tree.\n",
    "\n",
    "For features that were judged as non-impactful by the decision tree model, the partial dependency plots obtained using our GAM whitebox model validate the results. The variables ParentMaritalStatus (f(5)) and NrSiblings (s(8)) take on several values, but the factor function and spline function applied to them show a smooth function that indicates no positive or negative correlation between the number of siblings and test success. \n",
    "\n",
    "Similarly, the Partial Dependency Plot of parents' marital status represents a step function that indicates no clear correlation between this feature and the student's academic success. Finally, for the IsFirstChild and School_Bus variables, which are also step functions, we have distinct steps but obviously very large standard deviations, demonstrating that the impact of these features on the variable of interest is not clear-cut. \n",
    "\n",
    "Apart from that, the other features are also step functions, but the impact seems quite clear, associated with very low standard deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### B. ALE plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('../Dataset/df_processed.csv')\n",
    "X = df.drop('Grade', axis=1).copy()\n",
    "y = df['Grade'].copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Apply a numerical mapping for categorical columns\n",
    "categorical_mapping = {\n",
    "    'Gender': {'female': 0, 'male': 1},\n",
    "    'EthnicGroup': {'group A': 0, 'group B': 1, 'group C': 2, 'group D': 3, 'group E': 4},\n",
    "    'ParentEduc': {'some high school': 0, 'high school': 1, \"associate's degree\": 2, 'some college': 3, \"bachelor's degree\": 4, \"master's degree\": 5},\n",
    "    'LunchType': {'standard': 0, 'free/reduced': 1},\n",
    "    'ParentMaritalStatus': {'widowed': 0, 'divorced': 1, 'single': 2, 'married': 3},\n",
    "    'PracticeSport': {'sometimes': 0, 'regularly': 1, 'never': 2},\n",
    "    'WklyStudyHours': {'Less than 5 hours': 0, 'Between 5-10 hours': 1, 'More than 10 hours': 2}\n",
    "}\n",
    "\n",
    "columns_to_map = list(categorical_mapping.keys())\n",
    "X_train[columns_to_map] = X_train[columns_to_map].replace(categorical_mapping)\n",
    "X_test[columns_to_map] = X_test[columns_to_map].replace(categorical_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we plot ALE for numeric discrete features. We only have one, **NrSiblings**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALE for numeric discrete features\n",
    "ale_eff = ale(X=X_test, model=best_decision_tree, feature=[\"NrSiblings\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal result: insofar as for our decision tree model (surrogate), this NrSiblings feature is one of those that does not impact the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ALE_plot(feature_,df,est):\n",
    "  df_temp = df.copy()\n",
    "  df_temp[feature_] = df_temp[feature_].astype(int)\n",
    "  ale_eff = ale(X=df_temp, model=est, feature=[feature_], grid_size=25, include_CI=True, C=0.95)\n",
    "  del df_temp\n",
    "    \n",
    "categorical_features = ['Gender', 'EthnicGroup', 'ParentEduc', 'LunchType', 'TestPrep',\n",
    "       'ParentMaritalStatus', 'PracticeSport', 'IsFirstChild', 'School_Bus', 'WklyStudyHours']\n",
    "\n",
    "for feature in categorical_features : \n",
    "  ALE_plot(feature, X_test, best_decision_tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For categoricals or variables with discrete values the interpretation is similar and we also get the average difference in prediction, but instead of bins each value will be replaced once with the value before it and once with the value after it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compare Surrogate model performance with Black-box model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('../Dataset/df_processed.csv')\n",
    "X = df.drop('Grade', axis=1).copy()\n",
    "y = df['Grade'].copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Load the model from the file\n",
    "model_filename = '../Models/blackbox_model.pkl'\n",
    "blackbox_model = joblib.load(model_filename)\n",
    "y_pred_blackbox = blackbox_model.predict(X_test)\n",
    "y_pred_blackbox_train = blackbox_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

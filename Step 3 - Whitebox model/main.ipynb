{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Predictive performance of your blackbox model and white-box model implementation\n",
    "\n",
    "Content of this third step :\n",
    "1. Analyze the predictive performance of our blackbox model \n",
    "2. Implement a whitebox model \n",
    "3. Compare the blackbox model with the performance of whitebox models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pygam import LogisticGAM, s,  f\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Analyze the predictive performance of our blackbox model \n",
    "\n",
    "The chosen blackbox model is a catboost model, saved in this notebook. First, we load this catboost model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('../Dataset/df_processed.csv')\n",
    "X = df.drop('Grade', axis=1).copy()\n",
    "y = df['Grade'].copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the file\n",
    "model_filename = '../Models/blackbox_model.pkl'\n",
    "blackbox_model = joblib.load(model_filename)\n",
    "y_pred_blackbox = blackbox_model.predict(X_test)\n",
    "y_pred_blackbox_train = blackbox_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now analyse the performances of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluation metrics of the blackbox model :\")\n",
    "\n",
    "# Printing the evaluation metrics of this blackbox catboost model for test set \n",
    "accuracy_catboost_train = accuracy_score(y_train, y_pred_blackbox_train)\n",
    "precision_catboost_train = precision_score(y_train, y_pred_blackbox_train)\n",
    "recall_catboost_train = recall_score(y_train, y_pred_blackbox_train)\n",
    "f1_catboost_train = f1_score(y_train, y_pred_blackbox_train)\n",
    "\n",
    "print('Accuracy train score : ', np.round(accuracy_catboost_train, 4)*100, '%')\n",
    "print('Precision train score : ', np.round(precision_catboost_train, 4)*100, '%')\n",
    "print('Recall train score : ', np.round(recall_catboost_train, 4)*100, '%')\n",
    "print('F1 train score : ', np.round(f1_catboost_train, 4)*100, '%')\n",
    "print('\\n')\n",
    "\n",
    "# Printing the evaluation metrics of this blackbox catboost model for train set\n",
    "accuracy_catboost_test = accuracy_score(y_test, y_pred_blackbox)\n",
    "precision_catboost_test = precision_score(y_test, y_pred_blackbox)\n",
    "recall_catboost_test = recall_score(y_test, y_pred_blackbox)\n",
    "f1_catboost_test = f1_score(y_test, y_pred_blackbox)\n",
    "\n",
    "print('Accuracy test score : ', np.round(accuracy_catboost_test, 4)*100, '%')\n",
    "print('Precision test score : ', np.round(precision_catboost_test, 4)*100, '%')\n",
    "print('Recall test score : ', np.round(recall_catboost_test, 4)*100, '%')\n",
    "print('F1 test score : ', np.round(f1_catboost_test, 4)*100, '%')\n",
    "print('\\n')\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(3, 3))\n",
    "cm = confusion_matrix(y_test, y_pred_blackbox)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='PiYG', cbar=False, annot_kws={\"size\": 10}, cbar_kws={\"shrink\": 0.5}, linewidths=0.5, linecolor='white')\n",
    "plt.xlabel('Predictions', fontsize=10)\n",
    "plt.ylabel('True label', fontsize=10)\n",
    "plt.title('Confusion Matrix\\n of the Blackbox model (test set)', fontsize=11)\n",
    "plt.show()\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(3, 3))\n",
    "cm = confusion_matrix(y_train, y_pred_blackbox_train)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='PiYG', cbar=False, annot_kws={\"size\": 10}, cbar_kws={\"shrink\": 0.5}, linewidths=0.5, linecolor='white')\n",
    "plt.xlabel('Predictions', fontsize=10)\n",
    "plt.ylabel('True label', fontsize=10)\n",
    "plt.title('Confusion Matrix\\n of the Blackbox model (train set)', fontsize=11)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Blackbox model performance summary**\n",
    "\n",
    "For a threshold set at 0.5, its test accuracy remains fairly low, equal to 77.31%, meaning that around a third of individuals are misclassified. A closer look at the correlation matrix reveals that most of the test dataset's false predictions are due to false negatives (items predicted as 0 but whose true label is 1). Indeed, 19.18% of the test dataset are false negatives, for 3.5% false positives.\n",
    "\n",
    "So the performance of our blackbox models is not optimal, but the positive point that demonstrates that the hyperparameter tuning has certainly been well done is that the model doesn't seem to be overfitting. Accuracy, recall, precision and F1 scores are more or less the same for the train and the test set, guaranteeing consistent performance from our dataset-independent model blackbox."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement Withebox models\n",
    "\n",
    "In this step, we implement several model and keep the one having the best performances. For a model to be considered as a whitebox model, it needs two main characteristics : \n",
    "- The algorithm used is straightforward to understand, and we can clearly interpret how the input features are transformed into the output or target variable.\n",
    "- We can identify the most important features to predict the target variable, and those features are understandable.\n",
    "\n",
    "We mentionned in class 5 whitebox models easily interpretable. The one we can potentielly apply for our binary classification problem are the **Logistic Regression** (LR), **Decision Tree** (DT), **Generative Additive Models** (GAN), **Penalised Logistic Tree Regression** (PLTR). Let's implement several of them and keep the ones with the best performances to go further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a numerical mapping for categorical columns\n",
    "categorical_mapping = {\n",
    "    'Gender': {'female': 0, 'male': 1},\n",
    "    'EthnicGroup': {'group A': 0, 'group B': 1, 'group C': 2, 'group D': 3, 'group E': 4},\n",
    "    'ParentEduc': {'some high school': 0, 'high school': 1, \"associate's degree\": 2, 'some college': 3, \"bachelor's degree\": 4, \"master's degree\": 5},\n",
    "    'LunchType': {'standard': 0, 'free/reduced': 1},\n",
    "    'ParentMaritalStatus': {'widowed': 0, 'divorced': 1, 'single': 2, 'married': 3},\n",
    "    'PracticeSport': {'sometimes': 0, 'regularly': 1, 'never': 2},\n",
    "    'WklyStudyHours': {'Less than 5 hours': 0, 'Between 5-10 hours': 1, 'More than 10 hours': 2}\n",
    "}\n",
    "\n",
    "columns_to_map = list(categorical_mapping.keys())\n",
    "X_train_whitebox = X_train.copy()\n",
    "X_test_whitebox = X_test.copy()\n",
    "X_train_whitebox[columns_to_map] = X_train_whitebox[columns_to_map].replace(categorical_mapping)\n",
    "X_test_whitebox[columns_to_map] = X_test_whitebox[columns_to_map].replace(categorical_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty dataframe to store the results of our whitebox models\n",
    "df_whitebox_results = pd.DataFrame(columns=['Model name', 'Model parameters', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Whitebox model 1 : Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Implement logistic Regression \n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_whitebox, y_train)\n",
    "y_pred_logreg = logreg.predict_proba(X_test_whitebox)[:, 1]\n",
    "y_pred_logreg = (y_pred_logreg > 0.5).astype(int)\n",
    "\n",
    "# Compute evaluation metrics \n",
    "accuracy_logreg = accuracy_score(y_test, y_pred_logreg)\n",
    "precision_logreg = precision_score(y_test, y_pred_logreg)\n",
    "recall_logreg = recall_score(y_test, y_pred_logreg)\n",
    "f1_logreg = f1_score(y_test, y_pred_logreg)\n",
    "\n",
    "# Append the results to df_whitebox_results\n",
    "results_dict_logistic = {\n",
    "    'Model name': ['Logistic Regression'], 'Model parameters': [logreg.get_params()],\n",
    "    'Accuracy': [accuracy_logreg], 'Precision': [precision_logreg], 'Recall': [recall_logreg], 'F1 Score': [f1_logreg]\n",
    "    }\n",
    "df_whitebox_results = pd.concat([df_whitebox_results, pd.DataFrame(results_dict_logistic)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Whitebox model 2 : Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Implement Decision Tree - Without hyperparameters tuning\n",
    "decision_tree1 = DecisionTreeClassifier()\n",
    "decision_tree1.fit(X_train_whitebox, y_train)\n",
    "y_pred_dt1 = decision_tree1.predict(X_test_whitebox)\n",
    "\n",
    "# Calculate evaluation metrics for classification\n",
    "accuracy_dt1 = accuracy_score(y_test, y_pred_dt1)\n",
    "precision_dt1 = precision_score(y_test, y_pred_dt1)\n",
    "recall_dt1 = recall_score(y_test, y_pred_dt1)\n",
    "f1_dt1 = f1_score(y_test, y_pred_dt1)\n",
    "\n",
    "# Append the results to df_whitebox_results\n",
    "results_dict_dt = {\n",
    "    'Model name': ['Decision Tree - without Tuning'], 'Model parameters': [decision_tree1.get_params()],\n",
    "    'Accuracy': [accuracy_dt1], 'Precision': [precision_dt1], 'Recall': [recall_dt1], 'F1 Score': [f1_dt1]\n",
    "    }\n",
    "df_whitebox_results = pd.concat([df_whitebox_results, pd.DataFrame(results_dict_dt)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In next step with apply a bayesian hyperparameters tuning to tune the parameters of the decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 bis - Implement Decision Tree - With hyperparameters tuning\n",
    "\n",
    "# Objective function to minimize\n",
    "def objective(params):\n",
    "    decision_tree = DecisionTreeClassifier(\n",
    "        max_depth=params['max_depth'],\n",
    "        min_samples_leaf=params['min_samples_leaf'],\n",
    "        max_leaf_nodes=params['max_leaf_nodes'],\n",
    "        min_samples_split=params['min_samples_split'],\n",
    "        min_weight_fraction_leaf=params['min_weight_fraction_leaf'],\n",
    "    )\n",
    "    score = -cross_val_score(decision_tree, X_train_whitebox, y_train, cv=5, scoring='accuracy').mean()\n",
    "    return score\n",
    "\n",
    "# Space for hyperparameters gridsearch\n",
    "space = {\n",
    "    'max_depth': hp.randint('max_depth', 1, 13),\n",
    "    'min_samples_leaf': hp.randint('min_samples_leaf', 1, 11),\n",
    "    'max_leaf_nodes': hp.randint('max_leaf_nodes', 2, 100),\n",
    "    'min_samples_split': hp.randint('min_samples_split', 2, 21),\n",
    "    'min_weight_fraction_leaf': hp.uniform('min_weight_fraction_leaf', 0.0, 0.5),\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=500, trials=trials, rstate=np.random.default_rng(seed=42))\n",
    "print(\"Best Hyperparameters:\", best)\n",
    "\n",
    "best_max_depth = int(best['max_depth'])\n",
    "best_min_samples_leaf = int(best['min_samples_leaf'])\n",
    "best_max_leaf_nodes = int(best['max_leaf_nodes'])\n",
    "best_min_samples_split = int(best['min_samples_split'])\n",
    "best_min_weight_fraction_leaf = best['min_weight_fraction_leaf']\n",
    "\n",
    "# Fit the model and make predictions\n",
    "decision_tree2 = DecisionTreeClassifier(\n",
    "    max_depth=best_max_depth,\n",
    "    min_samples_leaf=best_min_samples_leaf,\n",
    "    max_leaf_nodes=best_max_leaf_nodes,\n",
    "    min_samples_split=best_min_samples_split,\n",
    "    min_weight_fraction_leaf=best_min_weight_fraction_leaf,\n",
    ")\n",
    "decision_tree2.fit(X_train_whitebox, y_train)\n",
    "y_pred_dt2 = decision_tree2.predict(X_test_whitebox)\n",
    "\n",
    "# Calculate evaluation metrics for classification\n",
    "accuracy_dt2 = accuracy_score(y_test, y_pred_dt2)\n",
    "precision_dt2 = precision_score(y_test, y_pred_dt2)\n",
    "recall_dt2 = recall_score(y_test, y_pred_dt2)\n",
    "f1_dt2 = f1_score(y_test, y_pred_dt2)\n",
    "\n",
    "# Append the results to df_whitebox_results\n",
    "results_dict_dt2 = {\n",
    "    'Model name': ['Decision Tree - with Tuning'], 'Model parameters': [decision_tree2.get_params()],\n",
    "    'Accuracy': [accuracy_dt2], 'Precision': [precision_dt2], 'Recall': [recall_dt2], 'F1 Score': [f1_dt2]\n",
    "    }\n",
    "df_whitebox_results = pd.concat([df_whitebox_results, pd.DataFrame(results_dict_dt2)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Whitebox model 3 : Generative additive model (GAM)\n",
    "\n",
    "\n",
    "GAN can be applied to **binary classification settings** as seen in the course (slide 39). The **additive logistic regression** will have the form : \n",
    "$$log(\\frac{\\mu(X)}{1-\\mu(X)}) = \\alpha + f_1(X_1) + … + f_p(X_p)$$\n",
    "\n",
    "The $f_i(X_i)$ denote **smooth**, **non-parametrics** functions. It could refer to any distribution from exponential family, such as Gaussian, binomial, Poisson etc...\n",
    "\n",
    "**Advantages on GAN models** : \n",
    "- It allows us to easily examine the **partial relationships** betwen the target variable and the features. Its additive nature ensures that the **marginal impact** of a single  variable **does not depend on the others** in the model.\n",
    "- The **non-linear fits** can potentially make more accurate predictions of the target. \n",
    "\n",
    "\n",
    "**Remark :** For the Python implementation, we can use the packages pyGAM, PiML, and statsmodels. The **pyGAM library** comes with different non parametric functions that we can apply to our features : \n",
    "- We apply a **spline terms** 's' for continuous numeric features (NbSiblings column is actually the only one)\n",
    "- And **factor terms** 'f' for categorical features (all the others)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Generative Additive Model - Without hyperparameters tuning\n",
    "n_splines = X_train_whitebox.shape[1] \n",
    "X_train_array = np.array(X_train_whitebox)\n",
    "y_train_array = np.array(y_train)\n",
    "\n",
    "# Number of siblings is the only continuous numerical variable, all others are categorical\n",
    "loggam_1 = LogisticGAM(f(0) + f(1) + f(2) + f(3) + f(4) + f(5) + f(6) + f(7) + s(8) + f(9) + f(10), constraints = 'none').gridsearch(X_train_array, y_train_array)\n",
    "predictions = loggam_1.predict(X_test_whitebox)\n",
    "y_pred_loggam1 = predictions*1\n",
    "\n",
    "# Calculate evaluation metrics for classification\n",
    "accuracy_loggam_1 = accuracy_score(y_test, y_pred_loggam1)\n",
    "precision_loggam_1 = precision_score(y_test, y_pred_loggam1)\n",
    "recall_loggam_1 = recall_score(y_test, y_pred_loggam1)\n",
    "f1_loggam_1 = f1_score(y_test, y_pred_loggam1)\n",
    "\n",
    "# Append the results to df_whitebox_results\n",
    "results_dict_loggam_1 = {\n",
    "    'Model name': ['GAM - without Tuning'], 'Model parameters': [loggam_1.get_params()],\n",
    "    'Accuracy': [accuracy_loggam_1], 'Precision': [precision_loggam_1], 'Recall': [recall_loggam_1], 'F1 Score': [f1_loggam_1]\n",
    "    }\n",
    "df_whitebox_results = pd.concat([df_whitebox_results, pd.DataFrame(results_dict_loggam_1)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda values are : \n",
    "- by default for spline terms $s()$, the penalty value on their 2nd derivative, which encourage the functions to be smoother\n",
    "- by default for factor terms $f()$, a L2 penalty (ridge) which encourages to take on smaller weight values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Generative Additive Model - With hyperparameters tuning\n",
    "n_splines = X_train_whitebox.shape[1] \n",
    "X_train_array = np.array(X_train_whitebox)\n",
    "y_train_array = np.array(y_train)\n",
    "n_features = X_train_whitebox.shape[1]\n",
    "\n",
    "# Regularization parameters to tune\n",
    "lams = np.random.rand(500, n_features)\n",
    "lams = lams * 6 - 3 \n",
    "lams = 10 ** lams \n",
    "\n",
    "# Number of siblings is the only continuous numerical variable, all others are categorical\n",
    "loggam_2 = LogisticGAM(f(0) + f(1) + f(2) + f(3) + f(4) + f(5) + f(6) + f(7) + s(8) + f(9) + f(10), constraints = 'none').gridsearch(X_train_array, y_train_array, lam=lams)\n",
    "predictions = loggam_2.predict(X_test_whitebox)\n",
    "y_pred_loggam_2 = predictions*1\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy_loggam_2 = accuracy_score(y_test, y_pred_loggam_2)\n",
    "precision_loggam_2 = precision_score(y_test, y_pred_loggam_2)\n",
    "recall_loggam_2 = recall_score(y_test, y_pred_loggam_2)\n",
    "f1_loggam_2 = f1_score(y_test, y_pred_loggam_2)\n",
    "\n",
    "# Append the results to df_whitebox_results\n",
    "results_dict_loggam_2 = {\n",
    "    'Model name': ['GAM - with Tuning'], 'Model parameters': [loggam_2.get_params()],\n",
    "    'Accuracy': [accuracy_loggam_2], 'Precision': [precision_loggam_2], 'Recall': [recall_loggam_2], 'F1 Score': [f1_loggam_2]\n",
    "    }\n",
    "df_whitebox_results = pd.concat([df_whitebox_results, pd.DataFrame(results_dict_loggam_2)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compare whitebox model performance with blackbox model \n",
    "\n",
    "In previous step, we implemented three whitebox models : **Logistic Regression** (LR), **Decision Tree** (DT), **Generative Additive Models** (GAN). Let's compare their results with our blackbox models results, and save the model having the best performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the Blackbox results to df_whitebox_results\n",
    "results_dict_blackbox = {\n",
    "    'Model name': ['Blackbox model'], 'Model parameters': [blackbox_model.get_params()],\n",
    "    'Accuracy': [accuracy_catboost_test], 'Precision': [precision_catboost_test], 'Recall': [recall_catboost_test], 'F1 Score': [f1_catboost_test]\n",
    "    }\n",
    "df_whitebox_results = pd.concat([df_whitebox_results, pd.DataFrame(results_dict_blackbox)], ignore_index=True)\n",
    "df_whitebox_results_to_print = df_whitebox_results.drop(columns=['Model parameters'], inplace=False).copy()\n",
    "df_whitebox_results_to_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all the confusion matrix\n",
    "model_names = ['Logistic Regression', 'Decision Tree', 'Decision Tree tuned', 'Logistic GAM', 'Logistic GAM tuned', 'Blackbox model']\n",
    "predictions = [y_pred_logreg, y_pred_dt1, y_pred_dt2, y_pred_loggam1, y_pred_loggam_2, y_pred_blackbox]\n",
    "\n",
    "fig, axes = plt.subplots(1, 6, figsize=(24, 4.5))\n",
    "for i in range(6):\n",
    "    ax = axes[i]\n",
    "    cm = confusion_matrix(y_test, predictions[i])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='PiYG', cbar=False, ax=ax,\n",
    "                annot_kws={\"size\": 13}, cbar_kws={\"shrink\": 0.5}, linewidths=0.5, linecolor='white')\n",
    "    ax.set_xlabel('Predictions', fontsize=12)\n",
    "    ax.set_ylabel('True label', fontsize=12)\n",
    "    ax.set_title(model_names[i], fontsize=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison of whitebox and blackbox models** performance\n",
    "\n",
    "If we compare the performance of our whitebox models with that of the blackbox, we can see that we achieve an accuracy on the test set very close to that of the blackbox catboost model, particularly with the OLS, the tuned decision Tree and the GAM. Indeed, these three models all have an accuracy of around 77%, approaching the 78% accuracy of our blackbox catboost model. \n",
    "\n",
    "Predictive performance is therefore very close, but we'll choose to save the GAM model. This is because it is more robust to overfitting than the decision tree, whose performance can vary greatly depending on the training dataset, and it also captures non-linear relationships between features and the variable of interest, which logistic regression does not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Saving the chosen whitebox model\n",
    "\n",
    "Of the various models we evaluated, we chose to continue with the **GAM (with Tuning)** model, which showed the **highest accuracy score** of all the whitebox models we tested. We therefore save this model and move on to try and explain the results of our catboost model based on this GAM whitebox model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best performance whitebox model to a file\n",
    "model_filename = '../Models/whitebox_model.pkl'\n",
    "joblib.dump(loggam_2, model_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

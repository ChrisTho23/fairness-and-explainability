{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTERPRETABILITY AND ALGORITHMIC FAIRNESSES - Spotify Top Hit Playlist (2020-2021) \n",
    "The goal of this project is to apply most of the techniques presented during this course. <br>\n",
    "(The pre-commit nbstripout has been installed to delete all notebook outputs before commiting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: kaggle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip:  cannot find or open students-exam-scores.zip, students-exam-scores.zip.zip or students-exam-scores.zip.ZIP.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'students-exam-scores.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/vitrac/Desktop/M2 HEC/14 - Algorithm Fairness/fairness-and-explainability/Step 1 - EDA/main.ipynb Cellule 4\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vitrac/Desktop/M2%20HEC/14%20-%20Algorithm%20Fairness/fairness-and-explainability/Step%201%20-%20EDA/main.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mkaggle datasets download -d desalegngeb/students-exam-scores\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vitrac/Desktop/M2%20HEC/14%20-%20Algorithm%20Fairness/fairness-and-explainability/Step%201%20-%20EDA/main.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39munzip students-exam-scores.zip\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/vitrac/Desktop/M2%20HEC/14%20-%20Algorithm%20Fairness/fairness-and-explainability/Step%201%20-%20EDA/main.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m os\u001b[39m.\u001b[39;49mremove(\u001b[39m\"\u001b[39;49m\u001b[39mstudents-exam-scores.zip\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'students-exam-scores.zip'"
     ]
    }
   ],
   "source": [
    "# Before running this line creata a Kaggle token, download the resulting kaggle.json and place the file in Users/\"User\"/.kaggle/kaggle.json \n",
    "!kaggle datasets download -d desalegngeb/students-exam-scores\n",
    "!unzip students-exam-scores.zip\n",
    "os.remove(\"students-exam-scores.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Expanded_data_with_more_features.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/vitrac/Desktop/M2 HEC/14 - Algorithm Fairness/fairness-and-explainability/Step 1 - EDA/main.ipynb Cellule 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/vitrac/Desktop/M2%20HEC/14%20-%20Algorithm%20Fairness/fairness-and-explainability/Step%201%20-%20EDA/main.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39mExpanded_data_with_more_features.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, index_col\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Expanded_data_with_more_features.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Expanded_data_with_more_features.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(\"Original_data_with_more_rows.csv\")\n",
    "os.remove(\"Expanded_data_with_more_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature information\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical analysis of numerical features\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfC = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle duplicates\n",
    "duplicate_rows_data = dfC[df.duplicated()]\n",
    "print(\"number of duplicate rows: \", duplicate_rows_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each column and count the number of distinct values\n",
    "for column in dfC.columns:\n",
    "    num_distinct_values = len(dfC[column].unique())\n",
    "    print(f\"{column}: {num_distinct_values} distinct values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping the Studyhours\n",
    "study_mapping = {\n",
    "    '< 5': 'Less than 5 hours',\n",
    "    '5 - 10': 'Between 5-10 hours',\n",
    "    '> 10': 'More than 10 hours'\n",
    "}\n",
    "\n",
    "# Mapping the IsFirstChild\n",
    "value_mapping = {\n",
    "    'no': 0,\n",
    "    'yes': 1\n",
    "}\n",
    "\n",
    "# Mapping the TestPrep\n",
    "test_mapping = {\n",
    "    'none': 0,\n",
    "    'completed': 1\n",
    "}\n",
    "\n",
    "# Mapping the Schoolbus\n",
    "bus_mapping = {\n",
    "    'private': 0,\n",
    "    'school_bus': 1\n",
    "}\n",
    "\n",
    "# Fixing the values in the column\n",
    "dfC['WklyStudyHours'] = dfC['WklyStudyHours'].map(study_mapping)\n",
    "dfC['IsFirstChild'] = dfC['IsFirstChild'].map(value_mapping)\n",
    "dfC['TestPrep'] = dfC['TestPrep'].map(test_mapping)\n",
    "dfC['TransportMeans'] = dfC['TransportMeans'].map(bus_mapping)\n",
    "\n",
    "# Rename the column from 'education' to 'degree'\n",
    "dfC.rename(columns={'TransportMeans': 'School_Bus'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treatment of missing values\n",
    "# Interpolate for numericial value\n",
    "dfC['NrSiblings'] = dfC['NrSiblings'].fillna(dfC['NrSiblings'].mode()[0])\n",
    "\n",
    "# Use Mode for categoricial columns\n",
    "dfC['EthnicGroup'] = dfC['EthnicGroup'].fillna(dfC['EthnicGroup'].mode()[0])\n",
    "dfC['WklyStudyHours'] = dfC['WklyStudyHours'].fillna(dfC['WklyStudyHours'].mode()[0])\n",
    "dfC['ParentEduc'] = df['ParentEduc'].fillna(df['ParentEduc'].mode()[0])\n",
    "dfC['ParentMaritalStatus'] = dfC['ParentMaritalStatus'].fillna(dfC['ParentMaritalStatus'].mode()[0])\n",
    "# Use Mode for binary columns\n",
    "dfC['IsFirstChild'] = dfC['IsFirstChild'].fillna(dfC['IsFirstChild'].mode()[0])\n",
    "dfC['PracticeSport'] = dfC['PracticeSport'].fillna(dfC['PracticeSport'].mode()[0])\n",
    "dfC['TestPrep'] = dfC['TestPrep'].fillna(dfC['TestPrep'].mode()[0])\n",
    "dfC['School_Bus'] = dfC['School_Bus'].fillna(dfC['School_Bus'].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfP = dfC.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfP[\"Grade\"] = (dfP[\"WritingScore\"] + dfP[\"ReadingScore\"] + dfP[\"MathScore\"])/3\n",
    "dfP.drop(columns=[\"WritingScore\", \"ReadingScore\", \"MathScore\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode grade into binary variable\n",
    "dfP[\"Grade\"] = (dfP[\"Grade\"] > dfP[\"Grade\"].quantile(0.75)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the specified columns to int\n",
    "flaot_to_int = ['TestPrep', 'IsFirstChild', 'NrSiblings', 'School_Bus']\n",
    "dfP[flaot_to_int] = dfP[flaot_to_int].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and four subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "# Iterate over the columns and create the distribution plots\n",
    "columns = ['Gender', 'EthnicGroup', 'ParentEduc', 'LunchType']\n",
    "for i, col in enumerate(columns):\n",
    "    ax = axs[i//2, i%2]\n",
    "    dfP[col].value_counts().plot(kind='bar', ax=ax)\n",
    "    ax.set_title(f\"Distribution of {col}\")\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the merged graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takeaways: <br>\n",
    "(1) Data set is balanced regarding gender <br>\n",
    "(2) Data set is unbalanced regarding ethnicity. More than one third of the students stem from ethnicity group C while only about 10% stem from ethnicity group A <br>\n",
    "(3) Data set is unbalanced regarding parent education. More than 30% of the students have parents that went to some college while only about 7% of students come from parents with a Master's degree. (Assumption only highest degree is counted) <br>\n",
    "(4) Data set is unbalanced regarding lunch type. About two third of the students have standard lunches while only one third of students benefit from free/reduced lunches (Context: Low-income children are eligible to receive reduced-price or free meals at school)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure\n",
    "fig, axs = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "# Create the bar plot\n",
    "bar_plot = sns.countplot(data=dfP, x='Grade', ax=axs)\n",
    "axs.set_title('Count of Each Grade')\n",
    "\n",
    "# Annotate the count on top of every bar\n",
    "for p in bar_plot.patches:\n",
    "    bar_plot.annotate(f'{p.get_height()}', \n",
    "                      (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                      ha='center', \n",
    "                      va='center', \n",
    "                      xytext=(0, 5), \n",
    "                      textcoords='offset points')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takeaways: <br>\n",
    "As per construction, one fourth of the studenst have good grades (Grade == 1) while three fourth of the class do not have good grades (Grade == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data for students that achieved Grade==1\n",
    "grade_1_data = dfP[dfP['Grade'] == 1]\n",
    "\n",
    "# List of categorical variables\n",
    "cat_vars = ['Gender', 'EthnicGroup', 'ParentEduc', 'TestPrep', 'LunchType', 'ParentMaritalStatus', \n",
    "            'PracticeSport', 'IsFirstChild', 'NrSiblings', 'School_Bus', 'WklyStudyHours']\n",
    "\n",
    "# Melt the DataFrame to have two columns: one for variable names and one for values\n",
    "melted_data = grade_1_data[cat_vars].melt()\n",
    "\n",
    "# Create a FacetGrid\n",
    "g = sns.FacetGrid(data=melted_data, col_wrap=3, col='variable', sharex=False, sharey=False, height=4)\n",
    "g = g.map(sns.countplot, 'value', palette='Set2')\n",
    "\n",
    "# Set axis labels, titles, and x-axis tick labels orientation\n",
    "g.set_axis_labels('Count', 'Category')\n",
    "g.set_titles('{col_name}')\n",
    "g.set_xticklabels(rotation=90)\n",
    "\n",
    "# Annotate the count on top of every bar\n",
    "for ax in g.axes.flat:\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{p.get_height()}', \n",
    "                    (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                    ha='center', \n",
    "                    va='center', \n",
    "                    xytext=(0, 5), \n",
    "                    textcoords='offset points')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair plot\n",
    "sns.pairplot(data=dfP, hue='Grade', diag_kind='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_v(x, y):\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    chi2 = stats.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r - ((r-1)**2)/(n-1)\n",
    "    kcorr = k - ((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n",
    "\n",
    "# Create a DataFrame to hold the Cramer's V values\n",
    "correlation_matrix = pd.DataFrame(index=dfP.columns, columns=dfP.columns)\n",
    "\n",
    "# Fill the DataFrame with Cramer's V values\n",
    "for i in dfP.columns:\n",
    "    for j in dfP.columns:\n",
    "        correlation_matrix.loc[i, j] = cramers_v(dfP[i], dfP[j])\n",
    "\n",
    "# Convert to numeric\n",
    "correlation_matrix = correlation_matrix.apply(pd.to_numeric)\n",
    "\n",
    "# Create the heatmap\n",
    "sns.heatmap(correlation_matrix, annot=False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset to ../Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfP.to_csv('../Dataset/df_processed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairexplain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
